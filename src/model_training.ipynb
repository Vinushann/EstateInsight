{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/preprocessed/preprocessed_data.csv')\n",
    "X = df.drop(['price'],axis='columns')\n",
    "y = df.price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8629132245229444"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# train the model using an Linear Regression\n",
    "lr_clf = LinearRegression()\n",
    "lr_clf.fit(X_train,y_train)\n",
    "# score method will tell you the score\n",
    "lr_clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8629132245229444"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# train the model using an Linear Regression\n",
    "lr_clf = LinearRegression()\n",
    "lr_clf.fit(X_train,y_train)\n",
    "# score method will tell you the score\n",
    "lr_clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:white'>Use K Fold cross validation to measure accuracy of our LinearRegression model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82702546 0.86027005 0.85322178 0.8436466  0.85481502]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8477957812447722"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K fold cross validation\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# shuffle split and each fold has random rows or records\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "\n",
    "scores_lr = cross_val_score(LinearRegression(), X, y, cv=cv)\n",
    "print(scores_lr) #In the result of it, we can see the gap between maximum and minimum is not big. Therefore, we don't need to do hyperparameter tuning.\n",
    "np.average(scores_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: K fold cv gives accuracy (only?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='color:white'>Find best model using GridSearchCV</h2>\n",
    "<p> Using the gridsearch it will tell you which best model and it will do the hyper parameter tunning as well </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear_regression</td>\n",
       "      <td>0.847951</td>\n",
       "      <td>{'fit_intercept': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lasso</td>\n",
       "      <td>0.726809</td>\n",
       "      <td>{'alpha': 2, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ridge</td>\n",
       "      <td>0.848050</td>\n",
       "      <td>{'alpha': 0.1, 'fit_intercept': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.756214</td>\n",
       "      <td>{'alpha': 0.1, 'l1_ratio': 0.9}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.715690</td>\n",
       "      <td>{'criterion': 'squared_error', 'splitter': 'be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model  best_score  \\\n",
       "0  linear_regression    0.847951   \n",
       "1              lasso    0.726809   \n",
       "2              ridge    0.848050   \n",
       "3         elasticnet    0.756214   \n",
       "4      decision_tree    0.715690   \n",
       "\n",
       "                                         best_params  \n",
       "0                           {'fit_intercept': False}  \n",
       "1                {'alpha': 2, 'selection': 'random'}  \n",
       "2             {'alpha': 0.1, 'fit_intercept': False}  \n",
       "3                    {'alpha': 0.1, 'l1_ratio': 0.9}  \n",
       "4  {'criterion': 'squared_error', 'splitter': 'be...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import pandas as pd\n",
    "\n",
    "def find_best_model_using_gridsearchcv(X, y):\n",
    "    algos = {\n",
    "        'linear_regression': {\n",
    "            'model': LinearRegression(),\n",
    "            'params': {\n",
    "                'fit_intercept': [True, False]\n",
    "            }\n",
    "        },\n",
    "        'lasso': {\n",
    "            'model': Lasso(),\n",
    "            'params': {\n",
    "                'alpha': [1, 2],\n",
    "                'selection': ['random', 'cyclic']\n",
    "            }\n",
    "        },\n",
    "        'ridge': {\n",
    "            'model': Ridge(),\n",
    "            'params': {\n",
    "                'alpha': [0.1, 1, 10],\n",
    "                'fit_intercept': [True, False]\n",
    "            }\n",
    "        },\n",
    "        'elasticnet': {\n",
    "            'model': ElasticNet(),\n",
    "            'params': {\n",
    "                'alpha': [0.1, 1, 10],\n",
    "                'l1_ratio': [0.1, 0.5, 0.9]\n",
    "            }\n",
    "        },\n",
    "        'decision_tree': {\n",
    "            'model': DecisionTreeRegressor(),\n",
    "            'params': {\n",
    "                'criterion': ['squared_error', 'friedman_mse'],\n",
    "                'splitter': ['best', 'random']\n",
    "            }\n",
    "        },\n",
    "        # 'random_forest': {\n",
    "        #     'model': RandomForestRegressor(),\n",
    "        #     'params': {\n",
    "        #         'n_estimators': [10, 50, 100],\n",
    "        #         'max_depth': [None, 10, 20],\n",
    "        #         'min_samples_split': [2, 5, 10]\n",
    "        #     }\n",
    "        # },\n",
    "        # 'gradient_boosting': {\n",
    "        #     'model': GradientBoostingRegressor(),\n",
    "        #     'params': {\n",
    "        #         'n_estimators': [50, 100, 200],\n",
    "        #         'learning_rate': [0.01, 0.1, 0.2],\n",
    "        #         'max_depth': [3, 5, 7]\n",
    "        #     }\n",
    "        # },\n",
    "        # 'svr': {\n",
    "        #     'model': SVR(),\n",
    "        #     'params': {\n",
    "        #         'kernel': ['linear', 'rbf'],\n",
    "        #         'C': [0.1, 1, 10],\n",
    "        #         'gamma': ['scale', 'auto']\n",
    "        #     }\n",
    "        # },\n",
    "        # 'knn': {\n",
    "        #     'model': KNeighborsRegressor(),\n",
    "        #     'params': {\n",
    "        #         'n_neighbors': [3, 5, 7],\n",
    "        #         'weights': ['uniform', 'distance']\n",
    "        #     }\n",
    "        # }\n",
    "    }\n",
    "\n",
    "    scores = []\n",
    "    \n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "\n",
    "    # this is a dictonarie and which is inside that previous function\n",
    "    for algo_name, config in algos.items():\n",
    "        #  this cv para is Cross Validation\n",
    "        gs = GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)\n",
    "        gs.fit(X, y)\n",
    "        # it will tell you the best params in particular run\n",
    "        scores.append({\n",
    "            'model': algo_name,\n",
    "            'best_score': gs.best_score_,\n",
    "            'best_params': gs.best_params_\n",
    "        })\n",
    "\n",
    "    # pd.DataFrame will print it as tabular format\n",
    "    return pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "\n",
    "find_best_model_using_gridsearchcv(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error 710.8059794200266\n",
      "root_mean_squared_error 26.660944833595575\n",
      "mean_absolute_error 16.156268053670473\n",
      "R-squared 0.8629615011589108\n",
      "Adjusted r2 score 0.8353033988180597\n",
      "mean_absolute_percentage_error 0.1987442425801653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# train the model using an Linear Regression\n",
    "lr_clf = LinearRegression(fit_intercept=False)\n",
    "\n",
    "lr_clf.fit(X_train,y_train)\n",
    "# score method will tell you the score\n",
    "lr_clf.score(X_test,y_test)\n",
    "\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error \n",
    "print(\"mean_squared_error\",mean_squared_error(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"root_mean_squared_error\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"mean_absolute_error\",mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R-squared\",r2_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "Adj_r2 = 1 - (1-r2_score(y_test, y_pred)) * (len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "print(\"Adjusted r2 score\",Adj_r2)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"mean_absolute_percentage_error\",mean_absolute_percentage_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error 711.4319490782134\n",
      "root_mean_squared_error 26.67268170016306\n",
      "mean_absolute_error 16.118159396313608\n",
      "R-squared 0.8628408185186944\n",
      "Adjusted r2 score 0.8351583591333479\n",
      "mean_absolute_percentage_error 0.1978118759491989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# train the model using an Linear Regression\n",
    "ridge_model = Ridge(alpha=0.1, fit_intercept=False) #{'alpha': 0.1, 'fit_intercept': False}\n",
    "\n",
    "ridge_model.fit(X_train,y_train)\n",
    "# score method will tell you the score\n",
    "ridge_model.score(X_test,y_test)\n",
    "\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "\n",
    "# for i in zip(y_test, y_pred):\n",
    "    # print(i)\n",
    "    # break\n",
    "\n",
    "from sklearn.metrics import mean_squared_error \n",
    "print(\"mean_squared_error\",mean_squared_error(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"root_mean_squared_error\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"mean_absolute_error\",mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R-squared\",r2_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "Adj_r2 = 1 - (1-r2_score(y_test, y_pred)) * (len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "print(\"Adjusted r2 score\",Adj_r2)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"mean_absolute_percentage_error\",mean_absolute_percentage_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_squared_error 1488.5784899841306\n",
      "root_mean_squared_error 38.582100642449866\n",
      "mean_absolute_error 23.254978303216955\n",
      "R-squared 0.7130123161864708\n",
      "Adjusted r2 score 0.6550903833237733\n",
      "mean_absolute_percentage_error 0.27505323769901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# lasso_model = Lasso()\n",
    "lasso_model = Lasso(alpha=2, selection='random')\n",
    "lasso_model.fit(X_train,y_train)\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error \n",
    "print(\"mean_squared_error\",mean_squared_error(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"root_mean_squared_error\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"mean_absolute_error\",mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R-squared\",r2_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "Adj_r2 = 1 - (1-r2_score(y_test, y_pred)) * (len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "print(\"Adjusted r2 score\",Adj_r2)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(\"mean_absolute_percentage_error\",mean_absolute_percentage_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering our target variable is not in 6 digits but in 2 digits(ie, 43 instead of 4,300,000)\n",
    "# good or bad based is denoted based on higher or lower(h, l)\n",
    "\n",
    "#          LR                                                   Lasso                                                 Lasso with best parameters\n",
    "         \n",
    "#          mean_squared_error 710.8059794200266                 mean_squared_error 1460.2194454865455                 mean_squared_error 1488.5784899841306                       \n",
    "# h        root_mean_squared_error 26.660944833595575           root_mean_squared_error 38.21281781662464             root_mean_squared_error 38.582100642449866                                 \n",
    "# l        mean_absolute_error 16.156268053670473               mean_absolute_error 23.157751000677226                mean_absolute_error 23.254978303216955                          \n",
    "# h        R-squared 0.8629615011589108                         R-squared 0.7184797447099163                          R-squared 0.7130123161864708      \n",
    "# h        Adjusted r2 score 0.8353033988180597                 Adjusted r2 score 0.661661287869808                   Adjusted r2 score 0.6550903833237733                     \n",
    "# l        mean_absolute_percentage_error 0.1987442425801653    mean_absolute_percentage_error 0.27512403601036683    mean_absolute_percentage_error 0.27505323769901                                                 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         Ridge\n",
    "        \n",
    "#         mean_squared_error 711.4319490782134\n",
    "# h       root_mean_squared_error 26.67268170016306\n",
    "# l       mean_absolute_error 16.118159396313608\n",
    "# h       R-squared 0.8628408185186944\n",
    "# h       Adjusted r2 score 0.8351583591333479\n",
    "# l       mean_absolute_percentage_error 0.1978118759491989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression\n",
      "MSE:  [1.04037871e+18 6.14280903e+18 3.10254371e+03 2.59734803e+03\n",
      " 2.68660785e+03]\n",
      "MAE:  [7.17737800e+08 3.95137557e+08 3.82973991e+01 4.06393232e+01\n",
      " 3.17669469e+01]\n",
      "R²:  [-1.56709478e+14 -1.08365632e+15  5.83719779e-01  4.92052806e-01\n",
      "  7.81392317e-01]\n",
      "\n",
      "Ridge\n",
      "MSE:  [3813.20579033 2491.47960968 3092.60610402 2586.2085067  2699.80012673]\n",
      "MAE:  [43.50104463 39.37307955 38.16269217 40.48289215 31.69074324]\n",
      "R²:  [0.42562695 0.56047671 0.58505315 0.49423129 0.78031887]\n",
      "\n",
      "Lasso\n",
      "MSE:  [2860.41668    1671.03093394 2140.52275421 1826.67115319 3317.28457973]\n",
      "MAE:  [25.9659583  22.52777026 21.51430367 24.08477084 32.83928104]\n",
      "R²:  [0.56914304 0.70521251 0.71279783 0.64276929 0.73007453]\n"
     ]
    }
   ],
   "source": [
    "#Since in aboves, the cross validation was not done when calculating the metrics such as mse and etc, we will now use cross validation to find y_pred and hences calculate the needed metrics++\n",
    "#Using kfold cross validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "print(\"Linear regression\")\n",
    "scoring = {'mse_my_value': 'neg_mean_squared_error', 'mae': 'neg_mean_absolute_error', 'r2': 'r2'}\n",
    "scores = cross_validate(lr_clf, X, y, cv=5, scoring=scoring)\n",
    "# print(scores)\n",
    "print(\"MSE: \", -scores['test_mse_my_value'])\n",
    "print(\"MAE: \", -scores['test_mae'])\n",
    "print(\"R²: \", scores['test_r2'])\n",
    "print()\n",
    "print(\"Ridge\")\n",
    "scoring = {'mse': 'neg_mean_squared_error', 'mae': 'neg_mean_absolute_error', 'r2': 'r2'}\n",
    "scores = cross_validate(ridge_model, X, y, cv=5, scoring=scoring)\n",
    "# print(scores)\n",
    "print(\"MSE: \", -scores['test_mse'])\n",
    "print(\"MAE: \", -scores['test_mae'])\n",
    "print(\"R²: \", scores['test_r2'])\n",
    "print()\n",
    "print(\"Lasso\")\n",
    "scoring = {'mse': 'neg_mean_squared_error', 'mae': 'neg_mean_absolute_error', 'r2': 'r2'}\n",
    "scores = cross_validate(lasso_model, X, y, cv=5, scoring=scoring)\n",
    "# print(scores)\n",
    "print(\"MSE: \", -scores['test_mse'])\n",
    "print(\"MAE: \", -scores['test_mae'])\n",
    "print(\"R²: \", scores['test_r2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In grid search cv, it does cross validation, but it considers only accuracy metric. But in here, the best hyperparameters are searched.\n",
    "In cross_validates, it ofc does cross validation, but considers other metrics too. But in here, the best hyperparameters are not searched.\n",
    "\tSo is there a method that can do that too?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    " predict the price of a house based on its location, square footage (sqft),\n",
    " number of bathrooms (bath), and number of bedrooms (bhk)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Find the Index of the Location:\n",
    "\n",
    "loc_index = np.where(X.columns=='location2')[0][0] finds the index corresponding to\n",
    "'location2' in X.columns, say it's 4.\n",
    "Initialize the Feature Array:\n",
    "\n",
    "x = np.zeros(len(X.columns)) creates an array of zeros, e.g., [0, 0, 0, 0, 0, 0, ...].\n",
    "Set the Sqft, Bath, and BHK Values:\n",
    "\n",
    "x[0] = 1200, x[1] = 2, x[2] = 3 sets the first three values in x, so x becomes [1200, 2, 3, 0, 0, 0, ...].\n",
    "Set the One-Hot Encoded Location:\n",
    "\n",
    "if loc_index >= 0: x[4] = 1 sets the value of the corresponding location feature\n",
    "(for 'location2') to 1, so x becomes [1200, 2, 3, 0, 1, 0, ...].\n",
    "Make the Prediction:\n",
    "\n",
    "The trained model lr_clf.predict([x]) is used to predict the price based on these features, and\n",
    "the predicted price is returned.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def predict_price(location,sqft,bath,bhk):    \n",
    "    loc_index = np.where(X.columns==location)[0][0]\n",
    "\n",
    "    x = np.zeros(len(X.columns))\n",
    "    \n",
    "    x[0] = sqft\n",
    "    x[1] = bath\n",
    "    x[2] = bhk\n",
    "    \n",
    "    if loc_index >= 0:\n",
    "        # we are setting 4th index value as 1(True) means it is existing in the table if the user entered\n",
    "        # new place it will become zero.\n",
    "        x[loc_index] = 1\n",
    "\n",
    "    print(x)\n",
    "    # The model expects input as a 2D array (i.e., a list of features), which is why [x] is used.\n",
    "    # [0] will be return prediction\n",
    "    return lr_clf.predict([x])[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
