{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/preprocessed/preprocessed_data.csv')\n",
    "X = df.drop(['price'],axis='columns')\n",
    "y = df.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split, ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Define models and expanded hyperparameters for GridSearchCV\n",
    "algos = {\n",
    "    'linear_regression': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {\n",
    "            'fit_intercept': [True, False],\n",
    "            'normalize': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'lasso': {\n",
    "        'model': Lasso(),\n",
    "        'params': {\n",
    "            'alpha': [0.1, 1, 10, 100],\n",
    "            'selection': ['random', 'cyclic'],\n",
    "            'max_iter': [1000, 5000]\n",
    "        }\n",
    "    },\n",
    "    'ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {\n",
    "            'alpha': [0.1, 1, 10, 100],\n",
    "            'fit_intercept': [True, False],\n",
    "            'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg'],\n",
    "            'max_iter': [1000, 5000]\n",
    "        }\n",
    "    },\n",
    "    'elasticnet': {\n",
    "        'model': ElasticNet(),\n",
    "        'params': {\n",
    "            'alpha': [0.1, 1, 10],\n",
    "            'l1_ratio': [0.1, 0.5, 0.9],\n",
    "            'max_iter': [1000, 5000]\n",
    "        }\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'model': DecisionTreeRegressor(),\n",
    "        'params': {\n",
    "            'criterion': ['squared_error', 'friedman_mse', 'absolute_error'],\n",
    "            'splitter': ['best', 'random'],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': RandomForestRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'model': GradientBoostingRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each model using GridSearchCV\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "scores = []\n",
    "best_model = None\n",
    "best_score = -np.inf\n",
    "\n",
    "for algo_name, config in algos.items():\n",
    "    gs = GridSearchCV(config['model'], config['params'], cv=cv, n_jobs=-1, return_train_score=False)\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = gs.best_estimator_.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(algo_name, 'has completed')\n",
    "    scores.append({\n",
    "        'model': algo_name,\n",
    "        'test_mse': mse,\n",
    "        'test_rmse': rmse,\n",
    "        'test_mae': mae,\n",
    "        'test_r2': r2,\n",
    "        'best_params': gs.best_params_\n",
    "    })\n",
    "    \n",
    "    # Keep track of the best model based on R² score\n",
    "    if r2 > best_score:\n",
    "        best_score = r2\n",
    "        best_model = gs.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame and display them\n",
    "results_df = pd.DataFrame(scores, columns=['model', 'test_mse', 'test_rmse', 'test_mae', 'test_r2', 'best_params'])\n",
    "results_df.sort_values(by='test_r2', ascending=False, inplace=True)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results (MSE, RMSE, MAE, and R² for each model)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot Mean Squared Error\n",
    "ax[0, 0].barh(results_df['model'], results_df['test_mse'], color='skyblue')\n",
    "ax[0, 0].set_xlabel('Mean Squared Error')\n",
    "ax[0, 0].set_title('Model Comparison - Mean Squared Error')\n",
    "\n",
    "# Plot Root Mean Squared Error\n",
    "ax[0, 1].barh(results_df['model'], results_df['test_rmse'], color='lightgreen')\n",
    "ax[0, 1].set_xlabel('Root Mean Squared Error')\n",
    "ax[0, 1].set_title('Model Comparison - Root Mean Squared Error')\n",
    "\n",
    "# Plot Mean Absolute Error\n",
    "ax[1, 0].barh(results_df['model'], results_df['test_mae'], color='lightcoral')\n",
    "ax[1, 0].set_xlabel('Mean Absolute Error')\n",
    "ax[1, 0].set_title('Model Comparison - Mean Absolute Error')\n",
    "\n",
    "# Plot R² Score\n",
    "ax[1, 1].barh(results_df['model'], results_df['test_r2'], color='orange')\n",
    "ax[1, 1].set_xlabel('R² Score')\n",
    "ax[1, 1].set_title('Model Comparison - R² Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_model.pkl')\n",
    "print(f\"The best model is {best_model} with an R² score of {best_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
